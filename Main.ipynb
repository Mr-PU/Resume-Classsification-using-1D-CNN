{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99338ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import pydot\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c5ba722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot_ng\n",
    "import graphviz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "040830cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2119d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3b91ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e38ecef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pydot_ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de7d3b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_model import plot_model\n",
    "import PyPDF2\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cecfff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('resume.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9b800bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 150, 100)          23233700  \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 148, 128)          38528     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 49, 128)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 49, 128)           0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 47, 128)           49280     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 25)                825       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,332,669\n",
      "Trainable params: 23,332,669\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60749aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5df64db",
   "metadata": {},
   "source": [
    "## Text Extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7675390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = open('cv.pdf', 'rb') #resume input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07e095d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfReader = PyPDF2.PdfReader(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "babf8657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Creator': 'Microsoft® Word 2016',\n",
       " '/CreationDate': \"D:20230111110518+00'00'\",\n",
       " '/Producer': 'www.ilovepdf.com',\n",
       " '/ModDate': 'D:20230111110518Z'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfReader.metadata #About PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1eccf4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages in the document: 2\n"
     ]
    }
   ],
   "source": [
    "x= (len(pdfReader.pages))\n",
    "print(\"Number of pages in the document:\",x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f4307de",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9943767",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page_number in range(x):\n",
    "    pdfReader = PyPDF2.PdfReader(pdf)\n",
    "    if pdfReader.is_encrypted:\n",
    "        pdfReader.decrypt('')\n",
    "    page = pdfReader.pages[page_number]\n",
    "    text += page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86b990cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Brandon Thomas, Data Scientist  \\n323 Knob Hill Apt. G  \\nSan Francisco, CA 00000  \\n415.555.1212 |  email@gmail.com  \\nwww.dataminer.com  \\n \\nProfessional Profile  \\nA former Ruby and Java programmer with newly acquired skills , an insatiable intellectual curiosity, and the \\nability to mine hidden gems located within large sets of structured, semi -structured and unstructured data. \\nAble to leverage a heavy dose of mathematics and applied statistics with visualization and a healthy  sense of \\nexploration.  \\nEducation  \\nGrad Certificate, Data Mining  2012 University of California, San Diego  \\nRelevant Courses:  Data Mining Methods and Techniques, Data Preparation for Data  \\nMining and Advanced Methods and Applications  \\nB.S. Computer Science  2009 San Francisco State University  \\nCore Competencies  \\nStrategic Thinking: Able to influence the strategic direction of the company by identifying opportunities in \\nlarge, rich data sets and creating and implementing data driven strategies that fuel growth includin g revenue \\nand profits.  \\nModeling: Design and implement statistical / predictive models and cutting edge algorithms utilizing diverse \\nsources of data to predict demand, risk and price elasticity. Experience with creating ETL processes to \\nsource and link data . \\nAnalytics: Utilize analytical applications like SAS to identify trends and relationships between different pieces \\nof data, draw appropriate conclusions and translate analytical findings into risk management and marketing \\nstrategies that drive value.  \\nDrive Enhancements: Develop tools and reports that help users access and analyze data resulting in higher \\nrevenues and margins and a better customer experience.  \\nCommunications and Project Management: Capable of turning dry analysis into an exciting story that \\ninfluences the direction of the business and communicating with diverse teams to take a project from start to \\nfinish. Collaborate with product teams to develop and support our internal data platform and to support \\nongoing analyses.  \\nSkills and Tools  \\n■NoSQL data stores (Cassandra, MongoDB)  \\n■Hadoop, MySQL, Big Table, MapReduce, SAS  \\n■Large -scale, distributed systems design and development  \\n■Scaling, performance and scheduling and ETL techniques  \\n■C, C++, Java, Ruby on Rails  \\nExperience  Cool Jeans    San Francisco,  CA \\nData Analyst 2012 -present  \\nWork closely with various teams across the company to identify and solve business challenges utilizing large \\nstructured, semi -structured, and unstructured data in a distributed processing environment. Develop a new \\npricing stra tegy for Total Jeans that boosted margins by 2 percent and analyzed customer buying habits \\nwhich correctly predicated the resurgence of dark blue denim giving us a jump on the competition.  \\n■Analyze large datasets to provide strategic direction to the compa ny. \\n■Perform quantitative analysis of product sales trends to recommend pricing decisions.  \\n■Conduct cost and benefit analysis on new ideas.  \\n■Scrutinize and track customer behavior to identify trends and unmet needs.  \\n■Develop statistical models to forecast inventory and procurement cycles.  \\n■Assist in developing internal tools for data analysis.  \\nProgrammer    2010 -2011  \\n■Coded, tested, debugged, implemented and documented apps using Java and Ruby.  \\n■Developed eCommerce solutions and social networking functionali ty. \\n■Designed, developed and maintained eCommerce and social networking applications.  \\n■Built report interfaces and data feeds.  \\n■Gathered and collected information from various programs, analyzed time requirements and prepared \\ndocumentation to change existi ng programs.  \\nProfessional Affiliations and Education  \\nNoSQL and Big Data Conference 2013  \\nHadoop Hackathon: Learn Map Reduce 2013  \\nMember, Silicon Valley Big Data Meetup  \\n \\n '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26e418fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.close() # closing the pdf file object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153a8dbc",
   "metadata": {},
   "source": [
    "## Pre Processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "828901fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub('http\\S+\\s*', ' ', text)  # remove URLs\n",
    "text = re.sub('RT|cc', ' ', text)  # remove RT and cc\n",
    "text = re.sub('#\\S+', '', text)  # remove hashtags\n",
    "text = re.sub('@\\S+', '  ', text)  # remove mentions\n",
    "text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', text)  # remove punctuation\n",
    "text = re.sub(r'[^\\x00-\\x7f]',r' ', text) \n",
    "text = re.sub('\\s+', ' ', text)  # remove extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46edfe06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3609"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "435ecc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "943f117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb9cccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14a23f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cachedStopWords = stopwords.words(\"english\")\n",
    "pattern = re.compile(r'\\b(' + r'|'.join(stop_words) + r')\\b\\s*')\n",
    "text = pattern.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2280ca17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "318e7834",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=[text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "330d6018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "624290cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PRASHANT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49275cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5145753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Brandon Thomas Data Scientist 323 Knob Hill Apt G San Francisco CA 00000 415 555 1212 email www dataminer com Professional Profile A former Ruby Java programmer newly acquired skills insatiable intellectual curiosity ability mine hidden gems located within large sets structured semi structured unstructured data Able leverage heavy dose mathematics applied statistics visualization healthy sense exploration Education Grad Certificate Data Mining 2012 University California San Diego Relevant Courses Data Mining Methods Techniques Data Preparation Data Mining Advanced Methods Applications B S Computer Science 2009 San Francisco State University Core Competencies Strategic Thinking Able influence strategic direction company identifying opportunities large rich data sets creating implementing data driven strategies fuel growth includin g revenue profits Modeling Design implement statistical predictive models cutting edge algorithms utilizing diverse sources data predict demand risk price elasticity Experience creating ETL processes source link data Analytics Utilize analytical applications like SAS identify trends relationships different pieces data draw appropriate conclusions translate analytical findings risk management marketing strategies drive value Drive Enhancements Develop tools reports help users ess analyze data resulting higher revenues margins better customer experience Communications Project Management Capable turning dry analysis exciting story influences direction business communicating diverse teams take project start finish Collaborate product teams develop support internal data platform support ongoing analyses Skills Tools NoSQL data stores Cassandra MongoDB Hadoop MySQL Big Table MapReduce SAS Large scale distributed systems design development Scaling performance scheduling ETL techniques C C Java Ruby Rails Experience Cool Jeans San Francisco CA Data Analyst 2012 present Work closely various teams across company identify solve business challenges utilizing large structured semi structured unstructured data distributed processing environment Develop new pricing stra tegy Total Jeans boosted margins 2 percent analyzed customer buying habits correctly predicated resurgence dark blue denim giving us jump competition Analyze large datasets provide strategic direction compa ny Perform quantitative analysis product sales trends recommend pricing decisions Conduct cost benefit analysis new ideas Scrutinize track customer behavior identify trends unmet needs Develop statistical models forecast inventory procurement cycles Assist developing internal tools data analysis Programmer 2010 2011 Coded tested debugged implemented documented apps using Java Ruby Developed eCommerce solutions social networking functionali ty Designed developed maintained eCommerce social networking applications Built report interfaces data feeds Gathered collected information various programs analyzed time requirements prepared documentation change existi ng programs Professional Affiliations Education NoSQL Big Data Conference 2013 Hadoop Hackathon Learn Map Reduce 2013 Member Silicon Valley Big Data Meetup \n"
     ]
    }
   ],
   "source": [
    "text_out = ' '.join([lemmatizer.lemmatize(w) for w in text])\n",
    "print(text_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d62c945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_out=[text_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "108dc724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'brandon': 36, 'thomas': 261, 'data': 73, 'scientist': 230, '323': 7, 'knob': 151, 'hill': 132, 'apt': 28, 'san': 224, 'francisco': 116, 'ca': 40, '00000': 0, '415': 8, '555': 9, '1212': 1, 'email': 102, 'www': 284, 'dataminer': 74, 'com': 51, 'professional': 200, 'profile': 201, 'former': 115, 'ruby': 222, 'java': 148, 'programmer': 203, 'newly': 177, 'acquired': 12, 'skills': 236, 'insatiable': 143, 'intellectual': 144, 'curiosity': 68, 'ability': 10, 'mine': 168, 'hidden': 130, 'gems': 120, 'located': 157, 'within': 282, 'large': 152, 'sets': 234, 'structured': 251, 'semi': 232, 'unstructured': 272, 'able': 11, 'leverage': 154, 'heavy': 128, 'dose': 93, 'mathematics': 164, 'applied': 25, 'statistics': 245, 'visualization': 281, 'healthy': 127, 'sense': 233, 'exploration': 110, 'education': 100, 'grad': 122, 'certificate': 44, 'mining': 169, '2012': 5, 'university': 270, 'california': 41, 'diego': 86, 'relevant': 212, 'courses': 66, 'methods': 167, 'techniques': 257, 'preparation': 191, 'advanced': 14, 'applications': 24, 'computer': 58, 'science': 229, '2009': 2, 'state': 243, 'core': 63, 'competencies': 56, 'strategic': 249, 'thinking': 260, 'influence': 140, 'direction': 88, 'company': 55, 'identifying': 135, 'opportunities': 182, 'rich': 220, 'creating': 67, 'implementing': 138, 'driven': 96, 'strategies': 250, 'fuel': 117, 'growth': 123, 'includin': 139, 'revenue': 218, 'profits': 202, 'modeling': 170, 'design': 80, 'implement': 136, 'statistical': 244, 'predictive': 190, 'models': 171, 'cutting': 70, 'edge': 99, 'algorithms': 16, 'utilizing': 277, 'diverse': 90, 'sources': 241, 'predict': 189, 'demand': 78, 'risk': 221, 'price': 194, 'elasticity': 101, 'experience': 109, 'etl': 106, 'processes': 196, 'source': 240, 'link': 156, 'analytics': 21, 'utilize': 276, 'analytical': 20, 'like': 155, 'sas': 225, 'identify': 134, 'trends': 267, 'relationships': 211, 'different': 87, 'pieces': 186, 'draw': 94, 'appropriate': 26, 'conclusions': 59, 'translate': 266, 'findings': 112, 'management': 159, 'marketing': 163, 'drive': 95, 'value': 279, 'enhancements': 103, 'develop': 82, 'tools': 263, 'reports': 214, 'help': 129, 'users': 274, 'ess': 105, 'analyze': 22, 'resulting': 216, 'higher': 131, 'revenues': 219, 'margins': 162, 'better': 32, 'customer': 69, 'communications': 53, 'project': 205, 'capable': 42, 'turning': 268, 'dry': 97, 'analysis': 18, 'exciting': 107, 'story': 247, 'influences': 141, 'business': 38, 'communicating': 52, 'teams': 256, 'take': 255, 'start': 242, 'finish': 113, 'collaborate': 49, 'product': 199, 'support': 252, 'internal': 146, 'platform': 187, 'ongoing': 181, 'analyses': 17, 'nosql': 179, 'stores': 246, 'cassandra': 43, 'mongodb': 172, 'hadoop': 126, 'mysql': 173, 'big': 33, 'table': 254, 'mapreduce': 161, 'scale': 226, 'distributed': 89, 'systems': 253, 'development': 85, 'scaling': 227, 'performance': 185, 'scheduling': 228, 'rails': 208, 'cool': 62, 'jeans': 149, 'analyst': 19, 'present': 193, 'work': 283, 'closely': 47, 'various': 280, 'across': 13, 'solve': 239, 'challenges': 45, 'processing': 197, 'environment': 104, 'new': 176, 'pricing': 195, 'stra': 248, 'tegy': 258, 'total': 264, 'boosted': 35, 'percent': 183, 'analyzed': 23, 'buying': 39, 'habits': 124, 'correctly': 64, 'predicated': 188, 'resurgence': 217, 'dark': 72, 'blue': 34, 'denim': 79, 'giving': 121, 'us': 273, 'jump': 150, 'competition': 57, 'datasets': 75, 'provide': 206, 'compa': 54, 'ny': 180, 'perform': 184, 'quantitative': 207, 'sales': 223, 'recommend': 209, 'decisions': 77, 'conduct': 60, 'cost': 65, 'benefit': 31, 'ideas': 133, 'scrutinize': 231, 'track': 265, 'behavior': 30, 'unmet': 271, 'needs': 174, 'forecast': 114, 'inventory': 147, 'procurement': 198, 'cycles': 71, 'assist': 29, 'developing': 84, '2010': 3, '2011': 4, 'coded': 48, 'tested': 259, 'debugged': 76, 'implemented': 137, 'documented': 92, 'apps': 27, 'using': 275, 'developed': 83, 'ecommerce': 98, 'solutions': 238, 'social': 237, 'networking': 175, 'functionali': 118, 'ty': 269, 'designed': 81, 'maintained': 158, 'built': 37, 'report': 213, 'interfaces': 145, 'feeds': 111, 'gathered': 119, 'collected': 50, 'information': 142, 'programs': 204, 'time': 262, 'requirements': 215, 'prepared': 192, 'documentation': 91, 'change': 46, 'existi': 108, 'ng': 178, 'affiliations': 15, 'conference': 61, '2013': 6, 'hackathon': 125, 'learn': 153, 'map': 160, 'reduce': 210, 'member': 166, 'silicon': 235, 'valley': 278, 'meetup': 165}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer() # tokenize and build vocab\n",
    "vectorizer.fit(text_out) # summarize\n",
    "print(vectorizer.vocabulary_) # encode document\n",
    "vector = vectorizer.transform(text_out).toarray() # summarize encoded vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7592204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  1,  1,  1,  2,  2,  1,  1,  1,  1,  2,  1,  1,  1,  1,\n",
       "         1,  1,  4,  1,  2,  1,  2,  2,  3,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  3,  1,  1,  1,  1,  2,  1,  2,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  2,  1,  3,  1,  1,  1, 20,  1,  1,  1,  1,  1,  1,\n",
       "         2,  1,  4,  2,  1,  1,  1,  1,  3,  2,  2,  1,  1,  1,  1,  2,\n",
       "         1,  1,  2,  1,  2,  1,  1,  1,  1,  1,  2,  1,  1,  3,  1,  1,\n",
       "         1,  1,  1,  1,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  1,\n",
       "         1,  1,  1,  1,  1,  1,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  2,  1,  3,  2,  1,  1,  5,  1,  1,  1,  1,  1,  1,  2,\n",
       "         1,  1,  2,  1,  1,  1,  1,  2,  1,  3,  1,  2,  1,  1,  1,  2,\n",
       "         2,  1,  1,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  2,  1,  1,  1,  2,  2,  1,  1,  2,  2,  2,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  3,  1,\n",
       "         4,  2,  1,  1,  1,  1,  1,  1,  2,  1,  2,  1,  2,  2,  1,  1,\n",
       "         1,  1,  1,  1,  2,  1,  1,  1,  1,  3,  2,  4,  2,  1,  1,  1,\n",
       "         3,  2,  1,  1,  1,  1,  1,  3,  1,  1,  1,  3,  1,  1,  2,  1,\n",
       "         2,  1,  1,  1,  1,  2,  1,  1,  2,  1,  1,  1,  1]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1bdcfebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e80d87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 285)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "480b52e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = vector[:,:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3abe0d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7ede5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = tf.convert_to_tensor(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8b9a420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 150])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57b41253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 150), dtype=int64, numpy=\n",
       "array([[ 1,  1,  1,  1,  1,  2,  2,  1,  1,  1,  1,  2,  1,  1,  1,  1,\n",
       "         1,  1,  4,  1,  2,  1,  2,  2,  3,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  3,  1,  1,  1,  1,  2,  1,  2,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  2,  1,  3,  1,  1,  1, 20,  1,  1,  1,  1,  1,  1,\n",
       "         2,  1,  4,  2,  1,  1,  1,  1,  3,  2,  2,  1,  1,  1,  1,  2,\n",
       "         1,  1,  2,  1,  2,  1,  1,  1,  1,  1,  2,  1,  1,  3,  1,  1,\n",
       "         1,  1,  1,  1,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  1,\n",
       "         1,  1,  1,  1,  1,  1,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  2,  1,  3,  2]], dtype=int64)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35029f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 168ms/step\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "40097ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 25)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "67de08e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.4272373e-08, 2.0306105e-08, 1.2299750e-04, 1.4073813e-08,\n",
       "        1.8655915e-07, 2.8897065e-10, 4.9575250e-05, 5.3642876e-04,\n",
       "        1.6318705e-02, 3.9368223e-07, 2.8893730e-08, 1.0516870e-10,\n",
       "        2.2156970e-12, 9.8246574e-01, 1.9082706e-11, 4.5024793e-11,\n",
       "        1.3256903e-12, 1.7043686e-07, 5.1313678e-09, 5.7558958e-10,\n",
       "        4.5994835e-04, 4.5721848e-05, 3.1061095e-10, 1.3350341e-07,\n",
       "        3.3940926e-09]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94759b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label_index = np.argmax(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b09735e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b2d85487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98246574"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5fde86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5868027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
